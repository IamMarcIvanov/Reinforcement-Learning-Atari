{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pong-try2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VPE8NymrUCv"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jXiD3cXrSd4",
        "outputId": "6ff9b810-3ea9-40e0-a55b-87624bfcd9f5"
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install tensorflow==2.4.1\n",
        "!pip install tensorboardX"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.7/dist-packages (56.0.0)\n",
            "Requirement already satisfied: tensorflow==2.4.1 in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.12)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.36.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.4.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.6.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.32.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.15.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.12.4)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.19.5)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.3.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (56.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.28.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.10.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.0.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (56.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48qWUfg1rWt8"
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD80oBEDrce-"
      },
      "source": [
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTAoRIsOrfgA",
        "outputId": "1fac6966-6c14-4162-d4e9-e3cad44dc772"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7ff846924dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqO_RnAJriCD"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cCSdRoQq0V4"
      },
      "source": [
        "import cv2\n",
        "import gym\n",
        "import gym.spaces\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * \\\n",
        "            0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(\n",
        "            img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
        "                                                dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(\n",
        "            self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    return ScaledFloatFrame(env)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skM-bXWirBSO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # Convolution layer\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Compute output shape of layer to pass into fully connected layer\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        out = self.conv_layers(torch.zeros(1, *shape))\n",
        "        return int(np.prod(out.size()))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        conv_out = self.conv_layers(inputs).view(inputs.size()[0], -1)\n",
        "        return self.fc_layers(conv_out)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return self.forward(inputs)\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYcawMLlsCJE",
        "outputId": "dfae5259-43f5-442b-b16e-3ae9584fe160"
      },
      "source": [
        "# import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "MEAN_REWARD_BOUND = 19.5\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 10**5\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.02\n",
        "\n",
        "\n",
        "TransitionTable = collections.namedtuple('TransitionTable',\n",
        "                                         field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "# Class to handle storing and randomly sampling state transitions\n",
        "\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), (BATCH_SIZE), replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), np.array(dones, dtype=np.uint8), np.array(next_states)\n",
        "\n",
        "\n",
        "# Class that chooses actions using a DQN policy to step through the environment\n",
        "class Agent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def step_env(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "        done_reward = None\n",
        "\n",
        "        # choose random action every once in a while\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # get possible actions\n",
        "            state_actions = np.array([self.state], copy=False)\n",
        "            state_vector = torch.tensor(state_actions).to(device)\n",
        "            # use network to predict values of possible actions\n",
        "            q_values_vector = net(state_vector)\n",
        "            # pick the action with the best value\n",
        "            _, action_value = torch.max(q_values_vector, dim=1)\n",
        "            action = int(action_value.item())\n",
        "\n",
        "        # step in the environment using the action chosen above\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        # save transition as experience to experience buffer\n",
        "        exp_tuple = TransitionTable(self.state, action, reward, is_done,\n",
        "                                    new_state)\n",
        "        self.exp_buffer.append(exp_tuple)\n",
        "        self.state = new_state\n",
        "\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward\n",
        "\n",
        "\n",
        "def compute_loss(batch, net, target_net, device=\"cpu\"):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "    # convert everything into a torch tensor so that we can compute derivatives\n",
        "    states_vector = torch.tensor(states).to(device)\n",
        "    next_states_vector = torch.tensor(next_states).to(device)\n",
        "    actions_vector = torch.tensor(actions).to(device)\n",
        "    rewards_vector = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.BoolTensor(dones).to(device)\n",
        "\n",
        "    # compute the predictions our model would make for this batch of state\n",
        "    # transitions (represented as tensors)\n",
        "    state_action_values = net(states_vector).gather(1,\n",
        "                                                    actions_vector.unsqueeze(-1)).squeeze(-1)\n",
        "    # compute the actual values we got for those transitions\n",
        "    next_state_values = target_net(next_states_vector).max(1)[0]\n",
        "    # make sure future values aren't being considered for end states\n",
        "    next_state_values[done_mask] = 0.0\n",
        "    next_state_values = next_state_values.detach()\n",
        "\n",
        "    # add the future reward of a state reached to the rewards of the current\n",
        "    # transition to correctly represent the actual value of the state reached\n",
        "    expected_state_action_values = next_state_values * GAMMA + rewards_vector\n",
        "\n",
        "    # compute the Mean Squared Error Loss between our predictions and the\n",
        "    # actual values of the transitions\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n",
        "# handle arguments to the python file\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\"--cuda\", default=False, action=\"store_true\",\n",
        "#                     help=\"Enable cuda\")\n",
        "# parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "#                     help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
        "# parser.add_argument(\"--reward\", type=float, default=MEAN_REWARD_BOUND,\n",
        "#                     help=\"Mean reward boundary for stop of training, default= %.2f\" %\n",
        "#                     MEAN_REWARD_BOUND)\n",
        "# args = parser.parse_args()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env = make_env(DEFAULT_ENV_NAME)\n",
        "\n",
        "# make two networks, one for local training, and update the target network\n",
        "# to it every once in a while. If you find this confusing you're not alone,\n",
        "# you just need to do some reading and googling and you'll get it with time\n",
        "local_model = DQN(env.observation_space.shape,\n",
        "                        env.action_space.n).to(device)\n",
        "target_model = DQN(env.observation_space.shape,\n",
        "                          env.action_space.n).to(device)\n",
        "\n",
        "# log all your results, ideally in a way thats easy to visualize\n",
        "logs = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
        "print(local_model)\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "# adam optimizer, just a fancier gradient descent algorithm\n",
        "optimizer = optim.Adam(local_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "total_rewards = []\n",
        "curr_frame_idx = 0\n",
        "prev_frame_idx = 0\n",
        "curr_time = time.time()\n",
        "best_mean_reward = None\n",
        "\n",
        "while True:\n",
        "    curr_frame_idx += 1\n",
        "\n",
        "    # probability of choosing a random action at this step\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - curr_frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    # step in the environment\n",
        "    reward = agent.step_env(local_model, epsilon, device=device)\n",
        "\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (curr_frame_idx - prev_frame_idx) / (time.time() - curr_time)\n",
        "        prev_frame_idx = curr_frame_idx\n",
        "        curr_time = time.time()\n",
        "        mean_reward = np.mean(total_rewards[-100:])\n",
        "        print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" %\n",
        "              (curr_frame_idx, len(total_rewards), mean_reward, epsilon, speed))\n",
        "        logs.add_scalar(\"epsilon\", epsilon, curr_frame_idx)\n",
        "        logs.add_scalar(\"speed\", speed, curr_frame_idx)\n",
        "        logs.add_scalar(\"reward_100\", mean_reward, curr_frame_idx)\n",
        "        logs.add_scalar(\"reward\", reward, curr_frame_idx)\n",
        "\n",
        "        # if the model is has shown consistent improvement, save the model\n",
        "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "            torch.save(local_model.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
        "            if best_mean_reward is not None:\n",
        "                print(\"Best mean reward updated %.3f -> %.3f, model saved\"\n",
        "                      % (best_mean_reward, mean_reward))\n",
        "            best_mean_reward = mean_reward\n",
        "        if mean_reward > MEAN_REWARD_BOUND:\n",
        "            print(\"Solved in %d frames!\" % curr_frame_idx)\n",
        "            break\n",
        "\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "\n",
        "    if curr_frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        target_model.load_state_dict(local_model.state_dict())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss_t = compute_loss(batch, local_model, target_model, device=device)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "logs.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DQN(\n",
            "  (conv_layers): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (fc_layers): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n",
            "841: done 1 games, mean reward -21.000, eps 0.99, speed 688.09 f/s\n",
            "1754: done 2 games, mean reward -21.000, eps 0.98, speed 679.95 f/s\n",
            "2775: done 3 games, mean reward -20.000, eps 0.97, speed 685.19 f/s\n",
            "Best mean reward updated -21.000 -> -20.000, model saved\n",
            "3537: done 4 games, mean reward -20.250, eps 0.96, speed 672.35 f/s\n",
            "4422: done 5 games, mean reward -20.200, eps 0.96, speed 688.34 f/s\n",
            "5367: done 6 games, mean reward -20.167, eps 0.95, speed 691.01 f/s\n",
            "6275: done 7 games, mean reward -20.286, eps 0.94, speed 684.13 f/s\n",
            "7206: done 8 games, mean reward -20.250, eps 0.93, speed 679.83 f/s\n",
            "8197: done 9 games, mean reward -20.222, eps 0.92, speed 673.34 f/s\n",
            "9081: done 10 games, mean reward -20.300, eps 0.91, speed 669.76 f/s\n",
            "10039: done 11 games, mean reward -20.273, eps 0.90, speed 503.04 f/s\n",
            "10889: done 12 games, mean reward -20.333, eps 0.89, speed 109.80 f/s\n",
            "11679: done 13 games, mean reward -20.385, eps 0.88, speed 109.36 f/s\n",
            "12725: done 14 games, mean reward -20.357, eps 0.87, speed 110.70 f/s\n",
            "13663: done 15 games, mean reward -20.400, eps 0.86, speed 110.40 f/s\n",
            "14629: done 16 games, mean reward -20.375, eps 0.85, speed 110.25 f/s\n",
            "15681: done 17 games, mean reward -20.294, eps 0.84, speed 110.38 f/s\n",
            "16607: done 18 games, mean reward -20.333, eps 0.83, speed 109.75 f/s\n",
            "17537: done 19 games, mean reward -20.316, eps 0.82, speed 110.04 f/s\n",
            "18545: done 20 games, mean reward -20.250, eps 0.81, speed 110.30 f/s\n",
            "19475: done 21 games, mean reward -20.286, eps 0.81, speed 109.70 f/s\n",
            "20526: done 22 games, mean reward -20.318, eps 0.79, speed 108.64 f/s\n",
            "21394: done 23 games, mean reward -20.304, eps 0.79, speed 109.09 f/s\n",
            "22156: done 24 games, mean reward -20.333, eps 0.78, speed 109.23 f/s\n",
            "23149: done 25 games, mean reward -20.320, eps 0.77, speed 110.72 f/s\n",
            "23989: done 26 games, mean reward -20.308, eps 0.76, speed 109.92 f/s\n",
            "24869: done 27 games, mean reward -20.333, eps 0.75, speed 109.21 f/s\n",
            "25769: done 28 games, mean reward -20.321, eps 0.74, speed 109.15 f/s\n",
            "26736: done 29 games, mean reward -20.310, eps 0.73, speed 109.51 f/s\n",
            "28061: done 30 games, mean reward -20.267, eps 0.72, speed 108.41 f/s\n",
            "28924: done 31 games, mean reward -20.258, eps 0.71, speed 107.02 f/s\n",
            "29884: done 32 games, mean reward -20.281, eps 0.70, speed 107.61 f/s\n",
            "30797: done 33 games, mean reward -20.273, eps 0.69, speed 108.14 f/s\n",
            "31647: done 34 games, mean reward -20.294, eps 0.68, speed 106.83 f/s\n",
            "32523: done 35 games, mean reward -20.314, eps 0.67, speed 106.61 f/s\n",
            "33498: done 36 games, mean reward -20.278, eps 0.67, speed 108.33 f/s\n",
            "34322: done 37 games, mean reward -20.297, eps 0.66, speed 107.59 f/s\n",
            "35190: done 38 games, mean reward -20.289, eps 0.65, speed 106.72 f/s\n",
            "36074: done 39 games, mean reward -20.308, eps 0.64, speed 106.85 f/s\n",
            "36984: done 40 games, mean reward -20.325, eps 0.63, speed 106.87 f/s\n",
            "37890: done 41 games, mean reward -20.341, eps 0.62, speed 105.60 f/s\n",
            "38954: done 42 games, mean reward -20.357, eps 0.61, speed 106.01 f/s\n",
            "39851: done 43 games, mean reward -20.372, eps 0.60, speed 106.77 f/s\n",
            "41118: done 44 games, mean reward -20.318, eps 0.59, speed 107.05 f/s\n",
            "42151: done 45 games, mean reward -20.289, eps 0.58, speed 106.06 f/s\n",
            "43233: done 46 games, mean reward -20.239, eps 0.57, speed 105.77 f/s\n",
            "44140: done 47 games, mean reward -20.255, eps 0.56, speed 105.06 f/s\n",
            "45317: done 48 games, mean reward -20.208, eps 0.55, speed 104.75 f/s\n",
            "46097: done 49 games, mean reward -20.224, eps 0.54, speed 105.10 f/s\n",
            "47118: done 50 games, mean reward -20.220, eps 0.53, speed 105.70 f/s\n",
            "48037: done 51 games, mean reward -20.216, eps 0.52, speed 105.71 f/s\n",
            "49182: done 52 games, mean reward -20.192, eps 0.51, speed 105.13 f/s\n",
            "50172: done 53 games, mean reward -20.208, eps 0.50, speed 105.67 f/s\n",
            "51031: done 54 games, mean reward -20.204, eps 0.49, speed 105.17 f/s\n",
            "51911: done 55 games, mean reward -20.218, eps 0.48, speed 105.35 f/s\n",
            "52819: done 56 games, mean reward -20.214, eps 0.47, speed 103.83 f/s\n",
            "54005: done 57 games, mean reward -20.211, eps 0.46, speed 103.30 f/s\n",
            "55228: done 58 games, mean reward -20.190, eps 0.45, speed 104.42 f/s\n",
            "56142: done 59 games, mean reward -20.203, eps 0.44, speed 104.90 f/s\n",
            "57018: done 60 games, mean reward -20.200, eps 0.43, speed 104.85 f/s\n",
            "58182: done 61 games, mean reward -20.164, eps 0.42, speed 104.99 f/s\n",
            "59121: done 62 games, mean reward -20.161, eps 0.41, speed 104.16 f/s\n",
            "60273: done 63 games, mean reward -20.175, eps 0.40, speed 103.76 f/s\n",
            "61695: done 64 games, mean reward -20.156, eps 0.38, speed 104.32 f/s\n",
            "62695: done 65 games, mean reward -20.169, eps 0.37, speed 103.03 f/s\n",
            "63633: done 66 games, mean reward -20.182, eps 0.36, speed 103.39 f/s\n",
            "64971: done 67 games, mean reward -20.164, eps 0.35, speed 103.88 f/s\n",
            "65968: done 68 games, mean reward -20.176, eps 0.34, speed 102.91 f/s\n",
            "67263: done 69 games, mean reward -20.159, eps 0.33, speed 102.96 f/s\n",
            "68466: done 70 games, mean reward -20.157, eps 0.32, speed 103.51 f/s\n",
            "69825: done 71 games, mean reward -20.127, eps 0.30, speed 102.38 f/s\n",
            "71076: done 72 games, mean reward -20.125, eps 0.29, speed 102.93 f/s\n",
            "72568: done 73 games, mean reward -20.096, eps 0.27, speed 101.75 f/s\n",
            "74077: done 74 games, mean reward -20.095, eps 0.26, speed 102.07 f/s\n",
            "75401: done 75 games, mean reward -20.093, eps 0.25, speed 101.64 f/s\n",
            "76606: done 76 games, mean reward -20.079, eps 0.23, speed 100.90 f/s\n",
            "77852: done 77 games, mean reward -20.078, eps 0.22, speed 99.62 f/s\n",
            "79293: done 78 games, mean reward -20.051, eps 0.21, speed 94.88 f/s\n",
            "80829: done 79 games, mean reward -20.038, eps 0.19, speed 93.11 f/s\n",
            "81981: done 80 games, mean reward -20.025, eps 0.18, speed 92.34 f/s\n",
            "83274: done 81 games, mean reward -20.012, eps 0.17, speed 94.80 f/s\n",
            "84689: done 82 games, mean reward -20.012, eps 0.15, speed 96.42 f/s\n",
            "86278: done 83 games, mean reward -19.988, eps 0.14, speed 96.45 f/s\n",
            "Best mean reward updated -20.000 -> -19.988, model saved\n",
            "87745: done 84 games, mean reward -19.988, eps 0.12, speed 98.62 f/s\n",
            "89565: done 85 games, mean reward -19.941, eps 0.10, speed 96.18 f/s\n",
            "Best mean reward updated -19.988 -> -19.941, model saved\n",
            "90942: done 86 games, mean reward -19.930, eps 0.09, speed 96.34 f/s\n",
            "Best mean reward updated -19.941 -> -19.930, model saved\n",
            "92570: done 87 games, mean reward -19.885, eps 0.07, speed 95.48 f/s\n",
            "Best mean reward updated -19.930 -> -19.885, model saved\n",
            "94353: done 88 games, mean reward -19.852, eps 0.06, speed 96.85 f/s\n",
            "Best mean reward updated -19.885 -> -19.852, model saved\n",
            "96345: done 89 games, mean reward -19.820, eps 0.04, speed 96.68 f/s\n",
            "Best mean reward updated -19.852 -> -19.820, model saved\n",
            "98320: done 90 games, mean reward -19.789, eps 0.02, speed 96.73 f/s\n",
            "Best mean reward updated -19.820 -> -19.789, model saved\n",
            "100186: done 91 games, mean reward -19.769, eps 0.02, speed 96.50 f/s\n",
            "Best mean reward updated -19.789 -> -19.769, model saved\n",
            "101848: done 92 games, mean reward -19.761, eps 0.02, speed 95.53 f/s\n",
            "Best mean reward updated -19.769 -> -19.761, model saved\n",
            "104069: done 93 games, mean reward -19.710, eps 0.02, speed 96.08 f/s\n",
            "Best mean reward updated -19.761 -> -19.710, model saved\n",
            "106304: done 94 games, mean reward -19.660, eps 0.02, speed 97.61 f/s\n",
            "Best mean reward updated -19.710 -> -19.660, model saved\n",
            "109356: done 95 games, mean reward -19.505, eps 0.02, speed 96.81 f/s\n",
            "Best mean reward updated -19.660 -> -19.505, model saved\n",
            "112865: done 96 games, mean reward -19.292, eps 0.02, speed 94.86 f/s\n",
            "Best mean reward updated -19.505 -> -19.292, model saved\n",
            "116135: done 97 games, mean reward -19.186, eps 0.02, speed 96.27 f/s\n",
            "Best mean reward updated -19.292 -> -19.186, model saved\n",
            "119810: done 98 games, mean reward -18.929, eps 0.02, speed 96.40 f/s\n",
            "Best mean reward updated -19.186 -> -18.929, model saved\n",
            "122486: done 99 games, mean reward -18.828, eps 0.02, speed 96.92 f/s\n",
            "Best mean reward updated -18.929 -> -18.828, model saved\n",
            "125802: done 100 games, mean reward -18.700, eps 0.02, speed 96.72 f/s\n",
            "Best mean reward updated -18.828 -> -18.700, model saved\n",
            "127719: done 101 games, mean reward -18.660, eps 0.02, speed 95.97 f/s\n",
            "Best mean reward updated -18.700 -> -18.660, model saved\n",
            "129605: done 102 games, mean reward -18.660, eps 0.02, speed 95.93 f/s\n",
            "132709: done 103 games, mean reward -18.570, eps 0.02, speed 96.16 f/s\n",
            "Best mean reward updated -18.660 -> -18.570, model saved\n",
            "134794: done 104 games, mean reward -18.550, eps 0.02, speed 96.03 f/s\n",
            "Best mean reward updated -18.570 -> -18.550, model saved\n",
            "137065: done 105 games, mean reward -18.560, eps 0.02, speed 95.10 f/s\n",
            "140647: done 106 games, mean reward -18.470, eps 0.02, speed 95.41 f/s\n",
            "Best mean reward updated -18.550 -> -18.470, model saved\n",
            "143712: done 107 games, mean reward -18.400, eps 0.02, speed 95.14 f/s\n",
            "Best mean reward updated -18.470 -> -18.400, model saved\n",
            "146639: done 108 games, mean reward -18.370, eps 0.02, speed 95.86 f/s\n",
            "Best mean reward updated -18.400 -> -18.370, model saved\n",
            "149825: done 109 games, mean reward -18.280, eps 0.02, speed 95.51 f/s\n",
            "Best mean reward updated -18.370 -> -18.280, model saved\n",
            "153402: done 110 games, mean reward -18.060, eps 0.02, speed 94.59 f/s\n",
            "Best mean reward updated -18.280 -> -18.060, model saved\n",
            "157110: done 111 games, mean reward -18.020, eps 0.02, speed 94.93 f/s\n",
            "Best mean reward updated -18.060 -> -18.020, model saved\n",
            "159271: done 112 games, mean reward -17.980, eps 0.02, speed 94.51 f/s\n",
            "Best mean reward updated -18.020 -> -17.980, model saved\n",
            "162094: done 113 games, mean reward -17.940, eps 0.02, speed 94.69 f/s\n",
            "Best mean reward updated -17.980 -> -17.940, model saved\n",
            "165151: done 114 games, mean reward -17.890, eps 0.02, speed 94.51 f/s\n",
            "Best mean reward updated -17.940 -> -17.890, model saved\n",
            "168859: done 115 games, mean reward -17.660, eps 0.02, speed 94.73 f/s\n",
            "Best mean reward updated -17.890 -> -17.660, model saved\n",
            "172868: done 116 games, mean reward -17.480, eps 0.02, speed 95.14 f/s\n",
            "Best mean reward updated -17.660 -> -17.480, model saved\n",
            "176413: done 117 games, mean reward -17.340, eps 0.02, speed 94.95 f/s\n",
            "Best mean reward updated -17.480 -> -17.340, model saved\n",
            "178071: done 118 games, mean reward -17.300, eps 0.02, speed 95.01 f/s\n",
            "Best mean reward updated -17.340 -> -17.300, model saved\n",
            "180420: done 119 games, mean reward -17.260, eps 0.02, speed 95.14 f/s\n",
            "Best mean reward updated -17.300 -> -17.260, model saved\n",
            "184180: done 120 games, mean reward -17.110, eps 0.02, speed 94.71 f/s\n",
            "Best mean reward updated -17.260 -> -17.110, model saved\n",
            "186820: done 121 games, mean reward -16.780, eps 0.02, speed 94.98 f/s\n",
            "Best mean reward updated -17.110 -> -16.780, model saved\n",
            "189841: done 122 games, mean reward -16.670, eps 0.02, speed 94.52 f/s\n",
            "Best mean reward updated -16.780 -> -16.670, model saved\n",
            "191617: done 123 games, mean reward -16.640, eps 0.02, speed 94.14 f/s\n",
            "Best mean reward updated -16.670 -> -16.640, model saved\n",
            "194813: done 124 games, mean reward -16.490, eps 0.02, speed 94.81 f/s\n",
            "Best mean reward updated -16.640 -> -16.490, model saved\n",
            "197230: done 125 games, mean reward -16.120, eps 0.02, speed 94.77 f/s\n",
            "Best mean reward updated -16.490 -> -16.120, model saved\n",
            "200735: done 126 games, mean reward -15.850, eps 0.02, speed 95.36 f/s\n",
            "Best mean reward updated -16.120 -> -15.850, model saved\n",
            "203542: done 127 games, mean reward -15.530, eps 0.02, speed 95.87 f/s\n",
            "Best mean reward updated -15.850 -> -15.530, model saved\n",
            "206655: done 128 games, mean reward -15.250, eps 0.02, speed 95.56 f/s\n",
            "Best mean reward updated -15.530 -> -15.250, model saved\n",
            "209960: done 129 games, mean reward -14.980, eps 0.02, speed 96.16 f/s\n",
            "Best mean reward updated -15.250 -> -14.980, model saved\n",
            "212336: done 130 games, mean reward -14.630, eps 0.02, speed 96.05 f/s\n",
            "Best mean reward updated -14.980 -> -14.630, model saved\n",
            "215823: done 131 games, mean reward -14.370, eps 0.02, speed 95.67 f/s\n",
            "Best mean reward updated -14.630 -> -14.370, model saved\n",
            "218016: done 132 games, mean reward -13.970, eps 0.02, speed 95.82 f/s\n",
            "Best mean reward updated -14.370 -> -13.970, model saved\n",
            "220128: done 133 games, mean reward -13.580, eps 0.02, speed 95.04 f/s\n",
            "Best mean reward updated -13.970 -> -13.580, model saved\n",
            "223113: done 134 games, mean reward -13.250, eps 0.02, speed 94.95 f/s\n",
            "Best mean reward updated -13.580 -> -13.250, model saved\n",
            "225496: done 135 games, mean reward -12.890, eps 0.02, speed 94.91 f/s\n",
            "Best mean reward updated -13.250 -> -12.890, model saved\n",
            "228798: done 136 games, mean reward -12.640, eps 0.02, speed 94.55 f/s\n",
            "Best mean reward updated -12.890 -> -12.640, model saved\n",
            "232019: done 137 games, mean reward -12.510, eps 0.02, speed 95.52 f/s\n",
            "Best mean reward updated -12.640 -> -12.510, model saved\n",
            "235457: done 138 games, mean reward -12.290, eps 0.02, speed 95.20 f/s\n",
            "Best mean reward updated -12.510 -> -12.290, model saved\n",
            "237711: done 139 games, mean reward -11.910, eps 0.02, speed 95.53 f/s\n",
            "Best mean reward updated -12.290 -> -11.910, model saved\n",
            "240509: done 140 games, mean reward -11.580, eps 0.02, speed 96.05 f/s\n",
            "Best mean reward updated -11.910 -> -11.580, model saved\n",
            "242812: done 141 games, mean reward -11.190, eps 0.02, speed 95.72 f/s\n",
            "Best mean reward updated -11.580 -> -11.190, model saved\n",
            "245529: done 142 games, mean reward -10.860, eps 0.02, speed 95.52 f/s\n",
            "Best mean reward updated -11.190 -> -10.860, model saved\n",
            "247979: done 143 games, mean reward -10.510, eps 0.02, speed 95.95 f/s\n",
            "Best mean reward updated -10.860 -> -10.510, model saved\n",
            "250804: done 144 games, mean reward -10.200, eps 0.02, speed 95.48 f/s\n",
            "Best mean reward updated -10.510 -> -10.200, model saved\n",
            "254557: done 145 games, mean reward -10.020, eps 0.02, speed 96.21 f/s\n",
            "Best mean reward updated -10.200 -> -10.020, model saved\n",
            "258852: done 146 games, mean reward -9.810, eps 0.02, speed 95.65 f/s\n",
            "Best mean reward updated -10.020 -> -9.810, model saved\n",
            "260845: done 147 games, mean reward -9.390, eps 0.02, speed 95.93 f/s\n",
            "Best mean reward updated -9.810 -> -9.390, model saved\n",
            "263343: done 148 games, mean reward -9.060, eps 0.02, speed 95.80 f/s\n",
            "Best mean reward updated -9.390 -> -9.060, model saved\n",
            "267633: done 149 games, mean reward -8.860, eps 0.02, speed 96.68 f/s\n",
            "Best mean reward updated -9.060 -> -8.860, model saved\n",
            "270821: done 150 games, mean reward -8.570, eps 0.02, speed 96.35 f/s\n",
            "Best mean reward updated -8.860 -> -8.570, model saved\n",
            "273269: done 151 games, mean reward -8.240, eps 0.02, speed 96.25 f/s\n",
            "Best mean reward updated -8.570 -> -8.240, model saved\n",
            "275889: done 152 games, mean reward -7.900, eps 0.02, speed 95.76 f/s\n",
            "Best mean reward updated -8.240 -> -7.900, model saved\n",
            "279122: done 153 games, mean reward -7.610, eps 0.02, speed 96.19 f/s\n",
            "Best mean reward updated -7.900 -> -7.610, model saved\n",
            "282581: done 154 games, mean reward -7.360, eps 0.02, speed 96.26 f/s\n",
            "Best mean reward updated -7.610 -> -7.360, model saved\n",
            "285626: done 155 games, mean reward -7.050, eps 0.02, speed 95.39 f/s\n",
            "Best mean reward updated -7.360 -> -7.050, model saved\n",
            "287941: done 156 games, mean reward -6.680, eps 0.02, speed 96.32 f/s\n",
            "Best mean reward updated -7.050 -> -6.680, model saved\n",
            "291166: done 157 games, mean reward -6.410, eps 0.02, speed 95.72 f/s\n",
            "Best mean reward updated -6.680 -> -6.410, model saved\n",
            "293378: done 158 games, mean reward -6.050, eps 0.02, speed 96.61 f/s\n",
            "Best mean reward updated -6.410 -> -6.050, model saved\n",
            "295863: done 159 games, mean reward -5.680, eps 0.02, speed 96.17 f/s\n",
            "Best mean reward updated -6.050 -> -5.680, model saved\n",
            "298819: done 160 games, mean reward -5.360, eps 0.02, speed 95.70 f/s\n",
            "Best mean reward updated -5.680 -> -5.360, model saved\n",
            "301383: done 161 games, mean reward -5.040, eps 0.02, speed 95.93 f/s\n",
            "Best mean reward updated -5.360 -> -5.040, model saved\n",
            "303955: done 162 games, mean reward -4.720, eps 0.02, speed 95.69 f/s\n",
            "Best mean reward updated -5.040 -> -4.720, model saved\n",
            "307560: done 163 games, mean reward -4.470, eps 0.02, speed 96.00 f/s\n",
            "Best mean reward updated -4.720 -> -4.470, model saved\n",
            "311533: done 164 games, mean reward -4.290, eps 0.02, speed 96.15 f/s\n",
            "Best mean reward updated -4.470 -> -4.290, model saved\n",
            "314996: done 165 games, mean reward -4.000, eps 0.02, speed 95.75 f/s\n",
            "Best mean reward updated -4.290 -> -4.000, model saved\n",
            "318689: done 166 games, mean reward -3.810, eps 0.02, speed 96.28 f/s\n",
            "Best mean reward updated -4.000 -> -3.810, model saved\n",
            "322405: done 167 games, mean reward -3.570, eps 0.02, speed 95.79 f/s\n",
            "Best mean reward updated -3.810 -> -3.570, model saved\n",
            "325401: done 168 games, mean reward -3.450, eps 0.02, speed 96.30 f/s\n",
            "Best mean reward updated -3.570 -> -3.450, model saved\n",
            "328614: done 169 games, mean reward -3.290, eps 0.02, speed 95.81 f/s\n",
            "Best mean reward updated -3.450 -> -3.290, model saved\n",
            "332170: done 170 games, mean reward -3.050, eps 0.02, speed 96.13 f/s\n",
            "Best mean reward updated -3.290 -> -3.050, model saved\n",
            "335773: done 171 games, mean reward -2.880, eps 0.02, speed 96.20 f/s\n",
            "Best mean reward updated -3.050 -> -2.880, model saved\n",
            "338930: done 172 games, mean reward -2.600, eps 0.02, speed 96.05 f/s\n",
            "Best mean reward updated -2.880 -> -2.600, model saved\n",
            "341727: done 173 games, mean reward -2.290, eps 0.02, speed 96.13 f/s\n",
            "Best mean reward updated -2.600 -> -2.290, model saved\n",
            "344268: done 174 games, mean reward -1.960, eps 0.02, speed 95.42 f/s\n",
            "Best mean reward updated -2.290 -> -1.960, model saved\n",
            "347819: done 175 games, mean reward -1.780, eps 0.02, speed 95.78 f/s\n",
            "Best mean reward updated -1.960 -> -1.780, model saved\n",
            "350652: done 176 games, mean reward -1.490, eps 0.02, speed 95.78 f/s\n",
            "Best mean reward updated -1.780 -> -1.490, model saved\n",
            "353081: done 177 games, mean reward -1.130, eps 0.02, speed 96.04 f/s\n",
            "Best mean reward updated -1.490 -> -1.130, model saved\n",
            "355719: done 178 games, mean reward -0.820, eps 0.02, speed 95.84 f/s\n",
            "Best mean reward updated -1.130 -> -0.820, model saved\n",
            "358455: done 179 games, mean reward -0.500, eps 0.02, speed 95.99 f/s\n",
            "Best mean reward updated -0.820 -> -0.500, model saved\n",
            "361558: done 180 games, mean reward -0.200, eps 0.02, speed 96.09 f/s\n",
            "Best mean reward updated -0.500 -> -0.200, model saved\n",
            "364654: done 181 games, mean reward 0.070, eps 0.02, speed 96.40 f/s\n",
            "Best mean reward updated -0.200 -> 0.070, model saved\n",
            "368315: done 182 games, mean reward 0.300, eps 0.02, speed 95.84 f/s\n",
            "Best mean reward updated 0.070 -> 0.300, model saved\n",
            "371617: done 183 games, mean reward 0.580, eps 0.02, speed 96.01 f/s\n",
            "Best mean reward updated 0.300 -> 0.580, model saved\n",
            "374946: done 184 games, mean reward 0.770, eps 0.02, speed 96.07 f/s\n",
            "Best mean reward updated 0.580 -> 0.770, model saved\n",
            "378470: done 185 games, mean reward 0.990, eps 0.02, speed 96.20 f/s\n",
            "Best mean reward updated 0.770 -> 0.990, model saved\n",
            "381751: done 186 games, mean reward 1.140, eps 0.02, speed 96.39 f/s\n",
            "Best mean reward updated 0.990 -> 1.140, model saved\n",
            "384739: done 187 games, mean reward 1.420, eps 0.02, speed 95.99 f/s\n",
            "Best mean reward updated 1.140 -> 1.420, model saved\n",
            "387123: done 188 games, mean reward 1.730, eps 0.02, speed 96.36 f/s\n",
            "Best mean reward updated 1.420 -> 1.730, model saved\n",
            "390473: done 189 games, mean reward 1.980, eps 0.02, speed 96.68 f/s\n",
            "Best mean reward updated 1.730 -> 1.980, model saved\n",
            "393123: done 190 games, mean reward 2.320, eps 0.02, speed 96.08 f/s\n",
            "Best mean reward updated 1.980 -> 2.320, model saved\n",
            "395313: done 191 games, mean reward 2.660, eps 0.02, speed 96.38 f/s\n",
            "Best mean reward updated 2.320 -> 2.660, model saved\n",
            "398447: done 192 games, mean reward 2.900, eps 0.02, speed 96.35 f/s\n",
            "Best mean reward updated 2.660 -> 2.900, model saved\n",
            "401154: done 193 games, mean reward 3.160, eps 0.02, speed 96.43 f/s\n",
            "Best mean reward updated 2.900 -> 3.160, model saved\n",
            "403682: done 194 games, mean reward 3.460, eps 0.02, speed 96.04 f/s\n",
            "Best mean reward updated 3.160 -> 3.460, model saved\n",
            "407472: done 195 games, mean reward 3.580, eps 0.02, speed 96.13 f/s\n",
            "Best mean reward updated 3.460 -> 3.580, model saved\n",
            "410207: done 196 games, mean reward 3.680, eps 0.02, speed 97.03 f/s\n",
            "Best mean reward updated 3.580 -> 3.680, model saved\n",
            "413381: done 197 games, mean reward 3.880, eps 0.02, speed 96.13 f/s\n",
            "Best mean reward updated 3.680 -> 3.880, model saved\n",
            "416517: done 198 games, mean reward 3.930, eps 0.02, speed 95.77 f/s\n",
            "Best mean reward updated 3.880 -> 3.930, model saved\n",
            "420425: done 199 games, mean reward 4.100, eps 0.02, speed 96.84 f/s\n",
            "Best mean reward updated 3.930 -> 4.100, model saved\n",
            "423374: done 200 games, mean reward 4.280, eps 0.02, speed 95.79 f/s\n",
            "Best mean reward updated 4.100 -> 4.280, model saved\n",
            "426702: done 201 games, mean reward 4.530, eps 0.02, speed 96.70 f/s\n",
            "Best mean reward updated 4.280 -> 4.530, model saved\n",
            "429484: done 202 games, mean reward 4.800, eps 0.02, speed 95.14 f/s\n",
            "Best mean reward updated 4.530 -> 4.800, model saved\n",
            "432286: done 203 games, mean reward 5.040, eps 0.02, speed 89.26 f/s\n",
            "Best mean reward updated 4.800 -> 5.040, model saved\n",
            "435067: done 204 games, mean reward 5.330, eps 0.02, speed 91.21 f/s\n",
            "Best mean reward updated 5.040 -> 5.330, model saved\n",
            "437741: done 205 games, mean reward 5.660, eps 0.02, speed 93.71 f/s\n",
            "Best mean reward updated 5.330 -> 5.660, model saved\n",
            "440840: done 206 games, mean reward 5.860, eps 0.02, speed 91.57 f/s\n",
            "Best mean reward updated 5.660 -> 5.860, model saved\n",
            "443870: done 207 games, mean reward 6.130, eps 0.02, speed 90.23 f/s\n",
            "Best mean reward updated 5.860 -> 6.130, model saved\n",
            "447022: done 208 games, mean reward 6.390, eps 0.02, speed 90.29 f/s\n",
            "Best mean reward updated 6.130 -> 6.390, model saved\n",
            "449806: done 209 games, mean reward 6.630, eps 0.02, speed 90.34 f/s\n",
            "Best mean reward updated 6.390 -> 6.630, model saved\n",
            "452714: done 210 games, mean reward 6.760, eps 0.02, speed 91.57 f/s\n",
            "Best mean reward updated 6.630 -> 6.760, model saved\n",
            "454964: done 211 games, mean reward 7.100, eps 0.02, speed 91.44 f/s\n",
            "Best mean reward updated 6.760 -> 7.100, model saved\n",
            "457558: done 212 games, mean reward 7.420, eps 0.02, speed 92.84 f/s\n",
            "Best mean reward updated 7.100 -> 7.420, model saved\n",
            "460195: done 213 games, mean reward 7.720, eps 0.02, speed 93.46 f/s\n",
            "Best mean reward updated 7.420 -> 7.720, model saved\n",
            "463005: done 214 games, mean reward 8.000, eps 0.02, speed 94.42 f/s\n",
            "Best mean reward updated 7.720 -> 8.000, model saved\n",
            "466041: done 215 games, mean reward 8.110, eps 0.02, speed 94.03 f/s\n",
            "Best mean reward updated 8.000 -> 8.110, model saved\n",
            "468427: done 216 games, mean reward 8.300, eps 0.02, speed 95.64 f/s\n",
            "Best mean reward updated 8.110 -> 8.300, model saved\n",
            "470843: done 217 games, mean reward 8.510, eps 0.02, speed 94.01 f/s\n",
            "Best mean reward updated 8.300 -> 8.510, model saved\n",
            "473477: done 218 games, mean reward 8.810, eps 0.02, speed 96.05 f/s\n",
            "Best mean reward updated 8.510 -> 8.810, model saved\n",
            "476109: done 219 games, mean reward 9.090, eps 0.02, speed 94.13 f/s\n",
            "Best mean reward updated 8.810 -> 9.090, model saved\n",
            "479330: done 220 games, mean reward 9.180, eps 0.02, speed 95.00 f/s\n",
            "Best mean reward updated 9.090 -> 9.180, model saved\n",
            "481776: done 221 games, mean reward 9.220, eps 0.02, speed 95.09 f/s\n",
            "Best mean reward updated 9.180 -> 9.220, model saved\n",
            "483913: done 222 games, mean reward 9.510, eps 0.02, speed 94.18 f/s\n",
            "Best mean reward updated 9.220 -> 9.510, model saved\n",
            "486706: done 223 games, mean reward 9.600, eps 0.02, speed 95.10 f/s\n",
            "Best mean reward updated 9.510 -> 9.600, model saved\n",
            "490216: done 224 games, mean reward 9.710, eps 0.02, speed 94.47 f/s\n",
            "Best mean reward updated 9.600 -> 9.710, model saved\n",
            "493364: done 225 games, mean reward 9.620, eps 0.02, speed 94.28 f/s\n",
            "496760: done 226 games, mean reward 9.610, eps 0.02, speed 94.76 f/s\n",
            "499395: done 227 games, mean reward 9.620, eps 0.02, speed 94.09 f/s\n",
            "501870: done 228 games, mean reward 9.690, eps 0.02, speed 94.55 f/s\n",
            "505610: done 229 games, mean reward 9.670, eps 0.02, speed 94.14 f/s\n",
            "508991: done 230 games, mean reward 9.570, eps 0.02, speed 94.37 f/s\n",
            "512371: done 231 games, mean reward 9.570, eps 0.02, speed 94.27 f/s\n",
            "514933: done 232 games, mean reward 9.490, eps 0.02, speed 94.47 f/s\n",
            "517431: done 233 games, mean reward 9.450, eps 0.02, speed 94.73 f/s\n",
            "520663: done 234 games, mean reward 9.420, eps 0.02, speed 94.85 f/s\n",
            "523311: done 235 games, mean reward 9.380, eps 0.02, speed 94.67 f/s\n",
            "526757: done 236 games, mean reward 9.340, eps 0.02, speed 95.00 f/s\n",
            "530307: done 237 games, mean reward 9.460, eps 0.02, speed 94.63 f/s\n",
            "532889: done 238 games, mean reward 9.580, eps 0.02, speed 95.43 f/s\n",
            "535419: done 239 games, mean reward 9.550, eps 0.02, speed 95.87 f/s\n",
            "538556: done 240 games, mean reward 9.470, eps 0.02, speed 95.45 f/s\n",
            "541580: done 241 games, mean reward 9.340, eps 0.02, speed 95.57 f/s\n",
            "544926: done 242 games, mean reward 9.230, eps 0.02, speed 95.39 f/s\n",
            "548349: done 243 games, mean reward 9.100, eps 0.02, speed 95.68 f/s\n",
            "551754: done 244 games, mean reward 9.020, eps 0.02, speed 94.87 f/s\n",
            "554682: done 245 games, mean reward 9.140, eps 0.02, speed 95.24 f/s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}