{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DRL_15_16_17_DQN_Pong.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40Yb47zJQglm"
      },
      "source": [
        "DEEP REINFORCEMENT LEARNING EXPLAINED - 15 - 16 - 17\n",
        "# **Deep Q-Network (DQN)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q40Fa7qM4_lE"
      },
      "source": [
        "OpenAI Pong"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA1Y5VCv20XZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08daaeb9-9e7d-41cd-949f-3d87eee2df57"
      },
      "source": [
        "import gym\n",
        "import gym.spaces\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\" \n",
        "test_env = gym.make(DEFAULT_ENV_NAME)\n",
        "print(test_env.action_space.n)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QDaXip14JBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79fb40e4-69ec-4ec9-c75d-656be03dfcc1"
      },
      "source": [
        "print(test_env.unwrapped.get_action_meanings())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uzLQLz04z2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4240f469-568c-438e-f517-9cfa540da2ba"
      },
      "source": [
        "print(test_env.observation_space.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(210, 160, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzcdmzIL5EMI"
      },
      "source": [
        "\n",
        "Type of hardware accelerator provided by Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjUM99rEKFNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193831fe-10d9-4f6d-e16d-dd5b548c39ce"
      },
      "source": [
        "!nvidia-smi "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Apr 20 08:50:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhmsqgrHikEl"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRcuJGVSQi6g"
      },
      "source": [
        "## OpenAI Gym Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPi1lHINMuSu"
      },
      "source": [
        "# Taken from \n",
        "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], \n",
        "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    return ScaledFloatFrame(env)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wznv9I1KR_I3"
      },
      "source": [
        "## The DQN model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6B8v-Qh5Ykk"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn        # Pytorch neural network package\n",
        "import torch.optim as optim  # Pytorch optimization package\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4S1I9xWMkf3"
      },
      "source": [
        "# Taken from \n",
        "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/dqn_model.py\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taYi5LZnIOqz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e09e5fa-f2d4-44a7-f03f-987110dec06d"
      },
      "source": [
        "test_env = make_env(DEFAULT_ENV_NAME)\n",
        "test_net = DQN(test_env.observation_space.shape, test_env.action_space.n).to(device)\n",
        "print(test_net)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhv3Yf-aW7UW"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPJl73Z1YTa4"
      },
      "source": [
        "Load Tensorboard extension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCBQhXLfNeUG"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb_f_onMXkpb"
      },
      "source": [
        "Import required modules and define the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGwHC9dyXoPd"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "\n",
        "MEAN_REWARD_BOUND = 19.0           \n",
        "\n",
        "gamma = 0.99                   \n",
        "batch_size = 32                \n",
        "replay_size = 10000            \n",
        "learning_rate = 1e-4           \n",
        "sync_target_frames = 1000      \n",
        "replay_start_size = 10000      \n",
        "\n",
        "eps_start=1.0\n",
        "eps_decay=.999985\n",
        "eps_min=0.02"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFaMmDKqYmo4"
      },
      "source": [
        "Experience replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y79CNYsjY4w0"
      },
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class ExperienceReplay:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQDV04ktY3xs"
      },
      "source": [
        "Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdAKFiMWZw90"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "\n",
        "        done_reward = None\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipurwYpa6iKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21a4b89-5957-412b-951a-2a991bc4c2b1"
      },
      "source": [
        "import datetime\n",
        "print(\">>>Training starts at \",datetime.datetime.now())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>Training starts at  2021-04-20 08:51:13.438897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgpmAtchZwM_"
      },
      "source": [
        "Main training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEoc2PWmM2mu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9791fd38-5f18-464b-f49a-2a2200169463"
      },
      "source": [
        "env = make_env(DEFAULT_ENV_NAME)\n",
        "\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
        " \n",
        "buffer = ExperienceReplay(replay_size)\n",
        "agent = Agent(env, buffer)\n",
        "\n",
        "epsilon = eps_start\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "total_rewards = []\n",
        "frame_idx = 0  \n",
        "\n",
        "best_mean_reward = None\n",
        "\n",
        "while True:\n",
        "        frame_idx += 1\n",
        "        epsilon = max(epsilon*eps_decay, eps_min)\n",
        "\n",
        "        reward = agent.play_step(net, epsilon, device=device)\n",
        "        if reward is not None:\n",
        "            total_rewards.append(reward)\n",
        "\n",
        "            mean_reward = np.mean(total_rewards[-100:])\n",
        "\n",
        "            print(\"%d:  %d games, mean reward %.3f, (epsilon %.2f)\" % (\n",
        "                frame_idx, len(total_rewards), mean_reward, epsilon))\n",
        "            \n",
        "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
        "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "                torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
        "                best_mean_reward = mean_reward\n",
        "                if best_mean_reward is not None:\n",
        "                    print(\"Best mean reward updated %.3f\" % (best_mean_reward))\n",
        "\n",
        "            if mean_reward > MEAN_REWARD_BOUND:\n",
        "                print(\"Solved in %d frames!\" % frame_idx)\n",
        "                break\n",
        "\n",
        "        if len(buffer) < replay_start_size:\n",
        "            continue\n",
        "\n",
        "        batch = buffer.sample(batch_size)\n",
        "        states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "        states_v = torch.tensor(states).to(device)\n",
        "        next_states_v = torch.tensor(next_states).to(device)\n",
        "        actions_v = torch.tensor(actions).to(device)\n",
        "        rewards_v = torch.tensor(rewards).to(device)\n",
        "        done_mask = torch.ByteTensor(dones).to(device)\n",
        "\n",
        "        state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        next_state_values = target_net(next_states_v).max(1)[0]\n",
        "\n",
        "        next_state_values[done_mask] = 0.0\n",
        "\n",
        "        next_state_values = next_state_values.detach()\n",
        "\n",
        "        expected_state_action_values = next_state_values * gamma + rewards_v\n",
        "\n",
        "        loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if frame_idx % sync_target_frames == 0:\n",
        "            target_net.load_state_dict(net.state_dict())\n",
        "       \n",
        "writer.close()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "897:  1 games, mean reward -21.000, (epsilon 0.99)\n",
            "Best mean reward updated -21.000\n",
            "1815:  2 games, mean reward -20.500, (epsilon 0.97)\n",
            "Best mean reward updated -20.500\n",
            "2626:  3 games, mean reward -20.667, (epsilon 0.96)\n",
            "3516:  4 games, mean reward -20.750, (epsilon 0.95)\n",
            "4437:  5 games, mean reward -20.600, (epsilon 0.94)\n",
            "5348:  6 games, mean reward -20.667, (epsilon 0.92)\n",
            "6256:  7 games, mean reward -20.714, (epsilon 0.91)\n",
            "7108:  8 games, mean reward -20.750, (epsilon 0.90)\n",
            "8037:  9 games, mean reward -20.667, (epsilon 0.89)\n",
            "8994:  10 games, mean reward -20.600, (epsilon 0.87)\n",
            "9876:  11 games, mean reward -20.636, (epsilon 0.86)\n",
            "10698:  12 games, mean reward -20.667, (epsilon 0.85)\n",
            "11643:  13 games, mean reward -20.615, (epsilon 0.84)\n",
            "12616:  14 games, mean reward -20.643, (epsilon 0.83)\n",
            "13438:  15 games, mean reward -20.667, (epsilon 0.82)\n",
            "14303:  16 games, mean reward -20.688, (epsilon 0.81)\n",
            "15267:  17 games, mean reward -20.588, (epsilon 0.80)\n",
            "16415:  18 games, mean reward -20.444, (epsilon 0.78)\n",
            "Best mean reward updated -20.444\n",
            "17267:  19 games, mean reward -20.474, (epsilon 0.77)\n",
            "18059:  20 games, mean reward -20.500, (epsilon 0.76)\n",
            "19147:  21 games, mean reward -20.381, (epsilon 0.75)\n",
            "Best mean reward updated -20.381\n",
            "20068:  22 games, mean reward -20.364, (epsilon 0.74)\n",
            "Best mean reward updated -20.364\n",
            "20892:  23 games, mean reward -20.391, (epsilon 0.73)\n",
            "21861:  24 games, mean reward -20.417, (epsilon 0.72)\n",
            "22623:  25 games, mean reward -20.440, (epsilon 0.71)\n",
            "23464:  26 games, mean reward -20.462, (epsilon 0.70)\n",
            "24424:  27 games, mean reward -20.444, (epsilon 0.69)\n",
            "25330:  28 games, mean reward -20.464, (epsilon 0.68)\n",
            "26111:  29 games, mean reward -20.483, (epsilon 0.68)\n",
            "27040:  30 games, mean reward -20.467, (epsilon 0.67)\n",
            "27980:  31 games, mean reward -20.484, (epsilon 0.66)\n",
            "29033:  32 games, mean reward -20.438, (epsilon 0.65)\n",
            "29883:  33 games, mean reward -20.455, (epsilon 0.64)\n",
            "30707:  34 games, mean reward -20.471, (epsilon 0.63)\n",
            "31620:  35 games, mean reward -20.486, (epsilon 0.62)\n",
            "32504:  36 games, mean reward -20.500, (epsilon 0.61)\n",
            "33266:  37 games, mean reward -20.514, (epsilon 0.61)\n",
            "34116:  38 games, mean reward -20.526, (epsilon 0.60)\n",
            "35102:  39 games, mean reward -20.513, (epsilon 0.59)\n",
            "35938:  40 games, mean reward -20.500, (epsilon 0.58)\n",
            "36700:  41 games, mean reward -20.512, (epsilon 0.58)\n",
            "37871:  42 games, mean reward -20.476, (epsilon 0.57)\n",
            "38781:  43 games, mean reward -20.488, (epsilon 0.56)\n",
            "39590:  44 games, mean reward -20.500, (epsilon 0.55)\n",
            "40519:  45 games, mean reward -20.489, (epsilon 0.54)\n",
            "41359:  46 games, mean reward -20.478, (epsilon 0.54)\n",
            "42140:  47 games, mean reward -20.489, (epsilon 0.53)\n",
            "43071:  48 games, mean reward -20.500, (epsilon 0.52)\n",
            "43979:  49 games, mean reward -20.510, (epsilon 0.52)\n",
            "44917:  50 games, mean reward -20.520, (epsilon 0.51)\n",
            "45832:  51 games, mean reward -20.510, (epsilon 0.50)\n",
            "46641:  52 games, mean reward -20.519, (epsilon 0.50)\n",
            "47422:  53 games, mean reward -20.528, (epsilon 0.49)\n",
            "48506:  54 games, mean reward -20.519, (epsilon 0.48)\n",
            "49286:  55 games, mean reward -20.527, (epsilon 0.48)\n",
            "50150:  56 games, mean reward -20.518, (epsilon 0.47)\n",
            "51115:  57 games, mean reward -20.491, (epsilon 0.46)\n",
            "52167:  58 games, mean reward -20.466, (epsilon 0.46)\n",
            "53019:  59 games, mean reward -20.475, (epsilon 0.45)\n",
            "53860:  60 games, mean reward -20.483, (epsilon 0.45)\n",
            "54958:  61 games, mean reward -20.459, (epsilon 0.44)\n",
            "55794:  62 games, mean reward -20.452, (epsilon 0.43)\n",
            "56556:  63 games, mean reward -20.460, (epsilon 0.43)\n",
            "57396:  64 games, mean reward -20.453, (epsilon 0.42)\n",
            "58195:  65 games, mean reward -20.462, (epsilon 0.42)\n",
            "59059:  66 games, mean reward -20.455, (epsilon 0.41)\n",
            "59877:  67 games, mean reward -20.463, (epsilon 0.41)\n",
            "60873:  68 games, mean reward -20.441, (epsilon 0.40)\n",
            "61801:  69 games, mean reward -20.435, (epsilon 0.40)\n",
            "62697:  70 games, mean reward -20.429, (epsilon 0.39)\n",
            "63519:  71 games, mean reward -20.437, (epsilon 0.39)\n",
            "64429:  72 games, mean reward -20.444, (epsilon 0.38)\n",
            "65265:  73 games, mean reward -20.438, (epsilon 0.38)\n",
            "66235:  74 games, mean reward -20.446, (epsilon 0.37)\n",
            "66997:  75 games, mean reward -20.453, (epsilon 0.37)\n",
            "67879:  76 games, mean reward -20.461, (epsilon 0.36)\n",
            "68823:  77 games, mean reward -20.468, (epsilon 0.36)\n",
            "69742:  78 games, mean reward -20.462, (epsilon 0.35)\n",
            "70532:  79 games, mean reward -20.468, (epsilon 0.35)\n",
            "71354:  80 games, mean reward -20.475, (epsilon 0.34)\n",
            "72236:  81 games, mean reward -20.481, (epsilon 0.34)\n",
            "73176:  82 games, mean reward -20.488, (epsilon 0.33)\n",
            "74058:  83 games, mean reward -20.494, (epsilon 0.33)\n",
            "74880:  84 games, mean reward -20.500, (epsilon 0.33)\n",
            "75642:  85 games, mean reward -20.506, (epsilon 0.32)\n",
            "76552:  86 games, mean reward -20.512, (epsilon 0.32)\n",
            "77434:  87 games, mean reward -20.517, (epsilon 0.31)\n",
            "78256:  88 games, mean reward -20.523, (epsilon 0.31)\n",
            "79126:  89 games, mean reward -20.517, (epsilon 0.31)\n",
            "79907:  90 games, mean reward -20.522, (epsilon 0.30)\n",
            "80853:  91 games, mean reward -20.527, (epsilon 0.30)\n",
            "81735:  92 games, mean reward -20.533, (epsilon 0.29)\n",
            "82525:  93 games, mean reward -20.538, (epsilon 0.29)\n",
            "83349:  94 games, mean reward -20.543, (epsilon 0.29)\n",
            "84130:  95 games, mean reward -20.547, (epsilon 0.28)\n",
            "84892:  96 games, mean reward -20.552, (epsilon 0.28)\n",
            "85672:  97 games, mean reward -20.557, (epsilon 0.28)\n",
            "86512:  98 games, mean reward -20.551, (epsilon 0.27)\n",
            "87274:  99 games, mean reward -20.556, (epsilon 0.27)\n",
            "88096:  100 games, mean reward -20.560, (epsilon 0.27)\n",
            "88904:  101 games, mean reward -20.560, (epsilon 0.26)\n",
            "89666:  102 games, mean reward -20.570, (epsilon 0.26)\n",
            "90490:  103 games, mean reward -20.570, (epsilon 0.26)\n",
            "91312:  104 games, mean reward -20.570, (epsilon 0.25)\n",
            "92295:  105 games, mean reward -20.560, (epsilon 0.25)\n",
            "93249:  106 games, mean reward -20.550, (epsilon 0.25)\n",
            "94058:  107 games, mean reward -20.550, (epsilon 0.24)\n",
            "94880:  108 games, mean reward -20.550, (epsilon 0.24)\n",
            "95816:  109 games, mean reward -20.560, (epsilon 0.24)\n",
            "96597:  110 games, mean reward -20.570, (epsilon 0.23)\n",
            "97359:  111 games, mean reward -20.570, (epsilon 0.23)\n",
            "98341:  112 games, mean reward -20.570, (epsilon 0.23)\n",
            "99225:  113 games, mean reward -20.580, (epsilon 0.23)\n",
            "100067:  114 games, mean reward -20.570, (epsilon 0.22)\n",
            "101129:  115 games, mean reward -20.570, (epsilon 0.22)\n",
            "101965:  116 games, mean reward -20.560, (epsilon 0.22)\n",
            "102765:  117 games, mean reward -20.580, (epsilon 0.21)\n",
            "103675:  118 games, mean reward -20.610, (epsilon 0.21)\n",
            "104592:  119 games, mean reward -20.600, (epsilon 0.21)\n",
            "105450:  120 games, mean reward -20.600, (epsilon 0.21)\n",
            "106240:  121 games, mean reward -20.630, (epsilon 0.20)\n",
            "107030:  122 games, mean reward -20.640, (epsilon 0.20)\n",
            "107947:  123 games, mean reward -20.630, (epsilon 0.20)\n",
            "108904:  124 games, mean reward -20.620, (epsilon 0.20)\n",
            "109846:  125 games, mean reward -20.600, (epsilon 0.19)\n",
            "110627:  126 games, mean reward -20.600, (epsilon 0.19)\n",
            "111611:  127 games, mean reward -20.600, (epsilon 0.19)\n",
            "112373:  128 games, mean reward -20.600, (epsilon 0.19)\n",
            "113135:  129 games, mean reward -20.600, (epsilon 0.18)\n",
            "113971:  130 games, mean reward -20.600, (epsilon 0.18)\n",
            "114871:  131 games, mean reward -20.590, (epsilon 0.18)\n",
            "115652:  132 games, mean reward -20.610, (epsilon 0.18)\n",
            "116414:  133 games, mean reward -20.610, (epsilon 0.17)\n",
            "117195:  134 games, mean reward -20.610, (epsilon 0.17)\n",
            "118003:  135 games, mean reward -20.610, (epsilon 0.17)\n",
            "118826:  136 games, mean reward -20.610, (epsilon 0.17)\n",
            "119650:  137 games, mean reward -20.610, (epsilon 0.17)\n",
            "120561:  138 games, mean reward -20.610, (epsilon 0.16)\n",
            "121351:  139 games, mean reward -20.620, (epsilon 0.16)\n",
            "122171:  140 games, mean reward -20.630, (epsilon 0.16)\n",
            "122933:  141 games, mean reward -20.630, (epsilon 0.16)\n",
            "123755:  142 games, mean reward -20.650, (epsilon 0.16)\n",
            "124545:  143 games, mean reward -20.650, (epsilon 0.15)\n",
            "125385:  144 games, mean reward -20.640, (epsilon 0.15)\n",
            "126209:  145 games, mean reward -20.650, (epsilon 0.15)\n",
            "127092:  146 games, mean reward -20.660, (epsilon 0.15)\n",
            "127854:  147 games, mean reward -20.660, (epsilon 0.15)\n",
            "128676:  148 games, mean reward -20.660, (epsilon 0.15)\n",
            "129438:  149 games, mean reward -20.660, (epsilon 0.14)\n",
            "130200:  150 games, mean reward -20.660, (epsilon 0.14)\n",
            "130990:  151 games, mean reward -20.670, (epsilon 0.14)\n",
            "131752:  152 games, mean reward -20.670, (epsilon 0.14)\n",
            "132514:  153 games, mean reward -20.670, (epsilon 0.14)\n",
            "133337:  154 games, mean reward -20.680, (epsilon 0.14)\n",
            "134155:  155 games, mean reward -20.680, (epsilon 0.13)\n",
            "135113:  156 games, mean reward -20.690, (epsilon 0.13)\n",
            "135875:  157 games, mean reward -20.710, (epsilon 0.13)\n",
            "136637:  158 games, mean reward -20.730, (epsilon 0.13)\n",
            "137399:  159 games, mean reward -20.730, (epsilon 0.13)\n",
            "138189:  160 games, mean reward -20.730, (epsilon 0.13)\n",
            "138951:  161 games, mean reward -20.750, (epsilon 0.12)\n",
            "139713:  162 games, mean reward -20.760, (epsilon 0.12)\n",
            "140475:  163 games, mean reward -20.760, (epsilon 0.12)\n",
            "141439:  164 games, mean reward -20.760, (epsilon 0.12)\n",
            "142377:  165 games, mean reward -20.760, (epsilon 0.12)\n",
            "143139:  166 games, mean reward -20.770, (epsilon 0.12)\n",
            "143920:  167 games, mean reward -20.770, (epsilon 0.12)\n",
            "144744:  168 games, mean reward -20.790, (epsilon 0.11)\n",
            "145534:  169 games, mean reward -20.800, (epsilon 0.11)\n",
            "146358:  170 games, mean reward -20.810, (epsilon 0.11)\n",
            "147176:  171 games, mean reward -20.810, (epsilon 0.11)\n",
            "147938:  172 games, mean reward -20.810, (epsilon 0.11)\n",
            "148778:  173 games, mean reward -20.820, (epsilon 0.11)\n",
            "149660:  174 games, mean reward -20.820, (epsilon 0.11)\n",
            "150693:  175 games, mean reward -20.820, (epsilon 0.10)\n",
            "151700:  176 games, mean reward -20.810, (epsilon 0.10)\n",
            "152518:  177 games, mean reward -20.810, (epsilon 0.10)\n",
            "153400:  178 games, mean reward -20.820, (epsilon 0.10)\n",
            "154190:  179 games, mean reward -20.820, (epsilon 0.10)\n",
            "155102:  180 games, mean reward -20.820, (epsilon 0.10)\n",
            "156020:  181 games, mean reward -20.800, (epsilon 0.10)\n",
            "156782:  182 games, mean reward -20.800, (epsilon 0.10)\n",
            "157544:  183 games, mean reward -20.800, (epsilon 0.09)\n",
            "158334:  184 games, mean reward -20.800, (epsilon 0.09)\n",
            "159124:  185 games, mean reward -20.800, (epsilon 0.09)\n",
            "159886:  186 games, mean reward -20.800, (epsilon 0.09)\n",
            "160708:  187 games, mean reward -20.800, (epsilon 0.09)\n",
            "161548:  188 games, mean reward -20.790, (epsilon 0.09)\n",
            "162372:  189 games, mean reward -20.800, (epsilon 0.09)\n",
            "163222:  190 games, mean reward -20.800, (epsilon 0.09)\n",
            "164194:  191 games, mean reward -20.800, (epsilon 0.09)\n",
            "165106:  192 games, mean reward -20.800, (epsilon 0.08)\n",
            "165988:  193 games, mean reward -20.800, (epsilon 0.08)\n",
            "166839:  194 games, mean reward -20.800, (epsilon 0.08)\n",
            "167601:  195 games, mean reward -20.800, (epsilon 0.08)\n",
            "168391:  196 games, mean reward -20.800, (epsilon 0.08)\n",
            "169231:  197 games, mean reward -20.790, (epsilon 0.08)\n",
            "169993:  198 games, mean reward -20.800, (epsilon 0.08)\n",
            "170783:  199 games, mean reward -20.800, (epsilon 0.08)\n",
            "171545:  200 games, mean reward -20.800, (epsilon 0.08)\n",
            "172427:  201 games, mean reward -20.800, (epsilon 0.08)\n",
            "173249:  202 games, mean reward -20.800, (epsilon 0.07)\n",
            "174039:  203 games, mean reward -20.800, (epsilon 0.07)\n",
            "174829:  204 games, mean reward -20.800, (epsilon 0.07)\n",
            "175653:  205 games, mean reward -20.820, (epsilon 0.07)\n",
            "176535:  206 games, mean reward -20.830, (epsilon 0.07)\n",
            "177385:  207 games, mean reward -20.830, (epsilon 0.07)\n",
            "178313:  208 games, mean reward -20.820, (epsilon 0.07)\n",
            "179075:  209 games, mean reward -20.820, (epsilon 0.07)\n",
            "179897:  210 games, mean reward -20.820, (epsilon 0.07)\n",
            "180721:  211 games, mean reward -20.820, (epsilon 0.07)\n",
            "181543:  212 games, mean reward -20.820, (epsilon 0.07)\n",
            "182365:  213 games, mean reward -20.820, (epsilon 0.06)\n",
            "183127:  214 games, mean reward -20.830, (epsilon 0.06)\n",
            "184012:  215 games, mean reward -20.830, (epsilon 0.06)\n",
            "184774:  216 games, mean reward -20.840, (epsilon 0.06)\n",
            "185536:  217 games, mean reward -20.840, (epsilon 0.06)\n",
            "186372:  218 games, mean reward -20.830, (epsilon 0.06)\n",
            "187134:  219 games, mean reward -20.840, (epsilon 0.06)\n",
            "187974:  220 games, mean reward -20.830, (epsilon 0.06)\n",
            "188798:  221 games, mean reward -20.830, (epsilon 0.06)\n",
            "189560:  222 games, mean reward -20.830, (epsilon 0.06)\n",
            "190504:  223 games, mean reward -20.840, (epsilon 0.06)\n",
            "191354:  224 games, mean reward -20.850, (epsilon 0.06)\n",
            "192144:  225 games, mean reward -20.870, (epsilon 0.06)\n",
            "192934:  226 games, mean reward -20.870, (epsilon 0.06)\n",
            "193916:  227 games, mean reward -20.870, (epsilon 0.05)\n",
            "194738:  228 games, mean reward -20.870, (epsilon 0.05)\n",
            "195574:  229 games, mean reward -20.860, (epsilon 0.05)\n",
            "196364:  230 games, mean reward -20.870, (epsilon 0.05)\n",
            "197145:  231 games, mean reward -20.880, (epsilon 0.05)\n",
            "198027:  232 games, mean reward -20.880, (epsilon 0.05)\n",
            "198849:  233 games, mean reward -20.880, (epsilon 0.05)\n",
            "199689:  234 games, mean reward -20.870, (epsilon 0.05)\n",
            "200587:  235 games, mean reward -20.870, (epsilon 0.05)\n",
            "201377:  236 games, mean reward -20.870, (epsilon 0.05)\n",
            "202139:  237 games, mean reward -20.870, (epsilon 0.05)\n",
            "202967:  238 games, mean reward -20.870, (epsilon 0.05)\n",
            "203729:  239 games, mean reward -20.870, (epsilon 0.05)\n",
            "204552:  240 games, mean reward -20.870, (epsilon 0.05)\n",
            "205375:  241 games, mean reward -20.870, (epsilon 0.05)\n",
            "206195:  242 games, mean reward -20.870, (epsilon 0.05)\n",
            "206957:  243 games, mean reward -20.870, (epsilon 0.04)\n",
            "207799:  244 games, mean reward -20.880, (epsilon 0.04)\n",
            "208561:  245 games, mean reward -20.880, (epsilon 0.04)\n",
            "209351:  246 games, mean reward -20.880, (epsilon 0.04)\n",
            "210174:  247 games, mean reward -20.880, (epsilon 0.04)\n",
            "211058:  248 games, mean reward -20.880, (epsilon 0.04)\n",
            "211868:  249 games, mean reward -20.880, (epsilon 0.04)\n",
            "212630:  250 games, mean reward -20.880, (epsilon 0.04)\n",
            "213420:  251 games, mean reward -20.880, (epsilon 0.04)\n",
            "214316:  252 games, mean reward -20.870, (epsilon 0.04)\n",
            "215138:  253 games, mean reward -20.870, (epsilon 0.04)\n",
            "215978:  254 games, mean reward -20.870, (epsilon 0.04)\n",
            "216800:  255 games, mean reward -20.870, (epsilon 0.04)\n",
            "217562:  256 games, mean reward -20.870, (epsilon 0.04)\n",
            "218324:  257 games, mean reward -20.870, (epsilon 0.04)\n",
            "219086:  258 games, mean reward -20.870, (epsilon 0.04)\n",
            "220032:  259 games, mean reward -20.850, (epsilon 0.04)\n",
            "220882:  260 games, mean reward -20.850, (epsilon 0.04)\n",
            "221644:  261 games, mean reward -20.850, (epsilon 0.04)\n",
            "222514:  262 games, mean reward -20.850, (epsilon 0.04)\n",
            "223276:  263 games, mean reward -20.850, (epsilon 0.04)\n",
            "224098:  264 games, mean reward -20.860, (epsilon 0.03)\n",
            "224981:  265 games, mean reward -20.860, (epsilon 0.03)\n",
            "225863:  266 games, mean reward -20.860, (epsilon 0.03)\n",
            "226745:  267 games, mean reward -20.860, (epsilon 0.03)\n",
            "227641:  268 games, mean reward -20.850, (epsilon 0.03)\n",
            "228403:  269 games, mean reward -20.850, (epsilon 0.03)\n",
            "229353:  270 games, mean reward -20.830, (epsilon 0.03)\n",
            "230175:  271 games, mean reward -20.830, (epsilon 0.03)\n",
            "230937:  272 games, mean reward -20.830, (epsilon 0.03)\n",
            "231699:  273 games, mean reward -20.830, (epsilon 0.03)\n",
            "232523:  274 games, mean reward -20.830, (epsilon 0.03)\n",
            "233285:  275 games, mean reward -20.830, (epsilon 0.03)\n",
            "234185:  276 games, mean reward -20.830, (epsilon 0.03)\n",
            "235007:  277 games, mean reward -20.830, (epsilon 0.03)\n",
            "235825:  278 games, mean reward -20.830, (epsilon 0.03)\n",
            "236587:  279 games, mean reward -20.830, (epsilon 0.03)\n",
            "237349:  280 games, mean reward -20.830, (epsilon 0.03)\n",
            "238292:  281 games, mean reward -20.850, (epsilon 0.03)\n",
            "239202:  282 games, mean reward -20.850, (epsilon 0.03)\n",
            "239992:  283 games, mean reward -20.850, (epsilon 0.03)\n",
            "240754:  284 games, mean reward -20.850, (epsilon 0.03)\n",
            "241516:  285 games, mean reward -20.850, (epsilon 0.03)\n",
            "242297:  286 games, mean reward -20.850, (epsilon 0.03)\n",
            "243059:  287 games, mean reward -20.850, (epsilon 0.03)\n",
            "243911:  288 games, mean reward -20.860, (epsilon 0.03)\n",
            "244673:  289 games, mean reward -20.860, (epsilon 0.03)\n",
            "245435:  290 games, mean reward -20.860, (epsilon 0.03)\n",
            "246373:  291 games, mean reward -20.860, (epsilon 0.02)\n",
            "247135:  292 games, mean reward -20.860, (epsilon 0.02)\n",
            "247985:  293 games, mean reward -20.860, (epsilon 0.02)\n",
            "248807:  294 games, mean reward -20.860, (epsilon 0.02)\n",
            "249569:  295 games, mean reward -20.860, (epsilon 0.02)\n",
            "250331:  296 games, mean reward -20.860, (epsilon 0.02)\n",
            "251093:  297 games, mean reward -20.870, (epsilon 0.02)\n",
            "251883:  298 games, mean reward -20.870, (epsilon 0.02)\n",
            "252645:  299 games, mean reward -20.870, (epsilon 0.02)\n",
            "253485:  300 games, mean reward -20.860, (epsilon 0.02)\n",
            "254247:  301 games, mean reward -20.860, (epsilon 0.02)\n",
            "255009:  302 games, mean reward -20.860, (epsilon 0.02)\n",
            "255850:  303 games, mean reward -20.860, (epsilon 0.02)\n",
            "256612:  304 games, mean reward -20.860, (epsilon 0.02)\n",
            "257374:  305 games, mean reward -20.860, (epsilon 0.02)\n",
            "258136:  306 games, mean reward -20.860, (epsilon 0.02)\n",
            "258898:  307 games, mean reward -20.860, (epsilon 0.02)\n",
            "259660:  308 games, mean reward -20.870, (epsilon 0.02)\n",
            "260422:  309 games, mean reward -20.870, (epsilon 0.02)\n",
            "261244:  310 games, mean reward -20.870, (epsilon 0.02)\n",
            "262006:  311 games, mean reward -20.870, (epsilon 0.02)\n",
            "262768:  312 games, mean reward -20.870, (epsilon 0.02)\n",
            "263590:  313 games, mean reward -20.870, (epsilon 0.02)\n",
            "264352:  314 games, mean reward -20.870, (epsilon 0.02)\n",
            "265114:  315 games, mean reward -20.870, (epsilon 0.02)\n",
            "265876:  316 games, mean reward -20.870, (epsilon 0.02)\n",
            "266638:  317 games, mean reward -20.870, (epsilon 0.02)\n",
            "267569:  318 games, mean reward -20.880, (epsilon 0.02)\n",
            "268331:  319 games, mean reward -20.880, (epsilon 0.02)\n",
            "269155:  320 games, mean reward -20.890, (epsilon 0.02)\n",
            "269977:  321 games, mean reward -20.890, (epsilon 0.02)\n",
            "270767:  322 games, mean reward -20.890, (epsilon 0.02)\n",
            "271529:  323 games, mean reward -20.890, (epsilon 0.02)\n",
            "272291:  324 games, mean reward -20.890, (epsilon 0.02)\n",
            "273113:  325 games, mean reward -20.890, (epsilon 0.02)\n",
            "273875:  326 games, mean reward -20.890, (epsilon 0.02)\n",
            "274757:  327 games, mean reward -20.900, (epsilon 0.02)\n",
            "275607:  328 games, mean reward -20.900, (epsilon 0.02)\n",
            "276491:  329 games, mean reward -20.910, (epsilon 0.02)\n",
            "277403:  330 games, mean reward -20.910, (epsilon 0.02)\n",
            "278165:  331 games, mean reward -20.910, (epsilon 0.02)\n",
            "278987:  332 games, mean reward -20.910, (epsilon 0.02)\n",
            "279810:  333 games, mean reward -20.910, (epsilon 0.02)\n",
            "280572:  334 games, mean reward -20.920, (epsilon 0.02)\n",
            "281352:  335 games, mean reward -20.920, (epsilon 0.02)\n",
            "282132:  336 games, mean reward -20.920, (epsilon 0.02)\n",
            "282894:  337 games, mean reward -20.920, (epsilon 0.02)\n",
            "283776:  338 games, mean reward -20.920, (epsilon 0.02)\n",
            "284538:  339 games, mean reward -20.920, (epsilon 0.02)\n",
            "285338:  340 games, mean reward -20.920, (epsilon 0.02)\n",
            "286100:  341 games, mean reward -20.920, (epsilon 0.02)\n",
            "286983:  342 games, mean reward -20.920, (epsilon 0.02)\n",
            "287745:  343 games, mean reward -20.920, (epsilon 0.02)\n",
            "288628:  344 games, mean reward -20.920, (epsilon 0.02)\n",
            "289452:  345 games, mean reward -20.920, (epsilon 0.02)\n",
            "290214:  346 games, mean reward -20.920, (epsilon 0.02)\n",
            "290995:  347 games, mean reward -20.920, (epsilon 0.02)\n",
            "291831:  348 games, mean reward -20.910, (epsilon 0.02)\n",
            "292593:  349 games, mean reward -20.910, (epsilon 0.02)\n",
            "293415:  350 games, mean reward -20.910, (epsilon 0.02)\n",
            "294205:  351 games, mean reward -20.910, (epsilon 0.02)\n",
            "295029:  352 games, mean reward -20.920, (epsilon 0.02)\n",
            "295819:  353 games, mean reward -20.920, (epsilon 0.02)\n",
            "296609:  354 games, mean reward -20.920, (epsilon 0.02)\n",
            "297389:  355 games, mean reward -20.920, (epsilon 0.02)\n",
            "298151:  356 games, mean reward -20.920, (epsilon 0.02)\n",
            "298941:  357 games, mean reward -20.920, (epsilon 0.02)\n",
            "299721:  358 games, mean reward -20.920, (epsilon 0.02)\n",
            "300483:  359 games, mean reward -20.940, (epsilon 0.02)\n",
            "301365:  360 games, mean reward -20.940, (epsilon 0.02)\n",
            "302127:  361 games, mean reward -20.940, (epsilon 0.02)\n",
            "302917:  362 games, mean reward -20.940, (epsilon 0.02)\n",
            "303739:  363 games, mean reward -20.940, (epsilon 0.02)\n",
            "304501:  364 games, mean reward -20.940, (epsilon 0.02)\n",
            "305430:  365 games, mean reward -20.930, (epsilon 0.02)\n",
            "306192:  366 games, mean reward -20.930, (epsilon 0.02)\n",
            "306954:  367 games, mean reward -20.930, (epsilon 0.02)\n",
            "307794:  368 games, mean reward -20.940, (epsilon 0.02)\n",
            "308556:  369 games, mean reward -20.940, (epsilon 0.02)\n",
            "309318:  370 games, mean reward -20.960, (epsilon 0.02)\n",
            "310080:  371 games, mean reward -20.960, (epsilon 0.02)\n",
            "310842:  372 games, mean reward -20.960, (epsilon 0.02)\n",
            "311604:  373 games, mean reward -20.960, (epsilon 0.02)\n",
            "312366:  374 games, mean reward -20.960, (epsilon 0.02)\n",
            "313128:  375 games, mean reward -20.960, (epsilon 0.02)\n",
            "313890:  376 games, mean reward -20.970, (epsilon 0.02)\n",
            "314652:  377 games, mean reward -20.970, (epsilon 0.02)\n",
            "315414:  378 games, mean reward -20.970, (epsilon 0.02)\n",
            "316176:  379 games, mean reward -20.970, (epsilon 0.02)\n",
            "316998:  380 games, mean reward -20.970, (epsilon 0.02)\n",
            "317880:  381 games, mean reward -20.970, (epsilon 0.02)\n",
            "318689:  382 games, mean reward -20.970, (epsilon 0.02)\n",
            "319513:  383 games, mean reward -20.970, (epsilon 0.02)\n",
            "320275:  384 games, mean reward -20.970, (epsilon 0.02)\n",
            "321065:  385 games, mean reward -20.970, (epsilon 0.02)\n",
            "321905:  386 games, mean reward -20.960, (epsilon 0.02)\n",
            "322743:  387 games, mean reward -20.960, (epsilon 0.02)\n",
            "323565:  388 games, mean reward -20.960, (epsilon 0.02)\n",
            "324327:  389 games, mean reward -20.960, (epsilon 0.02)\n",
            "325089:  390 games, mean reward -20.960, (epsilon 0.02)\n",
            "325851:  391 games, mean reward -20.960, (epsilon 0.02)\n",
            "326613:  392 games, mean reward -20.960, (epsilon 0.02)\n",
            "327393:  393 games, mean reward -20.960, (epsilon 0.02)\n",
            "328217:  394 games, mean reward -20.960, (epsilon 0.02)\n",
            "328979:  395 games, mean reward -20.960, (epsilon 0.02)\n",
            "329741:  396 games, mean reward -20.960, (epsilon 0.02)\n",
            "330563:  397 games, mean reward -20.960, (epsilon 0.02)\n",
            "331344:  398 games, mean reward -20.960, (epsilon 0.02)\n",
            "332226:  399 games, mean reward -20.960, (epsilon 0.02)\n",
            "332988:  400 games, mean reward -20.970, (epsilon 0.02)\n",
            "333750:  401 games, mean reward -20.970, (epsilon 0.02)\n",
            "334600:  402 games, mean reward -20.970, (epsilon 0.02)\n",
            "335362:  403 games, mean reward -20.970, (epsilon 0.02)\n",
            "336124:  404 games, mean reward -20.970, (epsilon 0.02)\n",
            "336886:  405 games, mean reward -20.970, (epsilon 0.02)\n",
            "337708:  406 games, mean reward -20.970, (epsilon 0.02)\n",
            "338470:  407 games, mean reward -20.970, (epsilon 0.02)\n",
            "339232:  408 games, mean reward -20.970, (epsilon 0.02)\n",
            "339994:  409 games, mean reward -20.970, (epsilon 0.02)\n",
            "340756:  410 games, mean reward -20.970, (epsilon 0.02)\n",
            "341546:  411 games, mean reward -20.970, (epsilon 0.02)\n",
            "342386:  412 games, mean reward -20.970, (epsilon 0.02)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4f196dad72e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-59b460e77f96>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZPkszw66cmO"
      },
      "source": [
        "print(\">>>Training ends at \",datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNH2N64k3QRz"
      },
      "source": [
        "Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKbcwfK321Hl"
      },
      "source": [
        "tensorboard  --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p0jvxoC3m5W"
      },
      "source": [
        "## Using the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLEfbkKl6AZV"
      },
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import collections\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "FPS = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m0Vm4Yp91ZI"
      },
      "source": [
        "Tunning the image rendering in colab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgpHXywd5SyZ"
      },
      "source": [
        "# Taken from \n",
        "# https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7\n",
        "\n",
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "!pip install pyvirtualdisplay==0.2.* \\\n",
        "             PyOpenGL==3.1.* \\\n",
        "             PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "!pip install gym[box2d]==0.17.*\n",
        "\n",
        "import pyvirtualdisplay\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvN4S8R53mJI"
      },
      "source": [
        "# Taken (partially) from \n",
        "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/03_dqn_play.py\n",
        "\n",
        "\n",
        "model='PongNoFrameskip-v4-best.dat'\n",
        "record_folder=\"RL_pong_video\"  \n",
        "visualize=True\n",
        "\n",
        "env = make_env(DEFAULT_ENV_NAME)\n",
        "if record_folder:\n",
        "        env = gym.wrappers.Monitor(env, record_folder, force=True)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n)\n",
        "net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n",
        "\n",
        "state = env.reset()\n",
        "total_reward = 0.0\n",
        "\n",
        "TRAINING_SIZE = 500000\n",
        "\n",
        "nos_frames = 0\n",
        "nos_games = 0\n",
        "while True:\n",
        "        start_ts = time.time()\n",
        "        if visualize:\n",
        "            env.render()\n",
        "            nos_frames += 1\n",
        "        state_v = torch.tensor(np.array([state], copy=False))\n",
        "        q_vals = net(state_v).data.numpy()[0]\n",
        "        action = np.argmax(q_vals)\n",
        "        \n",
        "        state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            nos_games += 1\n",
        "            print('{:<6} games done'.format(nos_games))\n",
        "            if nos_frames > TRAINING_SIZE:\n",
        "              break\n",
        "        if visualize:\n",
        "            delta = 1/FPS - (time.time() - start_ts)\n",
        "            if delta > 0:\n",
        "                time.sleep(delta)\n",
        "print(\"Total reward: %.2f\" % total_reward)\n",
        "\n",
        "if record_folder:\n",
        "        env.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}